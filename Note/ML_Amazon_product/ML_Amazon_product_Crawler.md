# ML_Amazon_product_Crawler.md

- [ML_Amazon_product_Crawler.md](file:///C:/Local/Work/ML_Name/Note/ML_Amazon_product/ML_Amazon_product_Crawler.md) )

[![GitHub Issues](https://img.shields.io/github/issues/zalandoresearch/flair.svg)](https://github.com/zalandoresearch/flair/issues)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)
[![Travis](https://img.shields.io/travis/zalandoresearch/flair.svg)](https://travis-ci.org/zalandoresearch/flair)

## Todo

[09-07]

- - 1.  how to run proxy :
    2.  how to get the scrappy run, get the old work run
    3.  how to get header run
    4.
    5.  build a ps1 file , give a amazon link and get the product info open it using code

## Vision

## Objective

- develop anti -anti crawler

#### Strategy.

[09-07]

1. Simple one : Run beautiful soup.

- - - bs_lib.py

2. More complex, long, comprehensive one runs crappy.

- develop anti -anti crawler

# Content
## Basic 

### Basic run command 

scrappy crawl simple -L ERROR  
- get the old work run

### how to get header run

- Multiple headers
  // STUB

### How to run proxy

// STUB
https proxys

// STUB





### setup logging activity.

### use meta to transfer data 

### use meta to transfer data 



## Reference
- [Scrapy 2.2 documentation](https://docs.scrapy.org/en/latest/index.html)
- [Scrapy 2.2 documentation: -logging ](https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging-settings)
- [selectors](https://docs.scrapy.org/en/latest/topics/selectors.html)
- [Using your browserâ€™s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html#topics-developer-tools)

